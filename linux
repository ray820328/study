《Linux内核设计与实现》
《深入理解Linux内核》
《深入理解Linux虚拟内存管理》
《深入理解LINUX网络内幕》

共享内存（上）
https://www.ibm.com/developerworks/cn/linux/l-ipc/part5/index1.html

strace -tt -f  ./serverd

perf、 gprof 等程序的性能分析工具
性能分析工具几个因素： 
工具本身消耗的cpu大小：valgrind本身cpu消耗非常大，不适合使用于服务器负载较大时的性能分析，只适合于性能是线性增长的情况并作低负载测试。Oprofile、perf等系统内核支持的一般cpu消耗较低。 
能否用于运营环境的性能分析：Gprof是使用编译时嵌入代码的方式，需要程序正常退出才能得到统计结果，oprofile需要重编系统内核支持，运营环境下使用不方便。 
可视化操作是否简便等
基于功能上考虑。Gprof不能统计内核调用消耗，perf、vtune等基于硬件性能计数器的方式可以提供多种事件的统计分析结果，
perf工具指定branch-misses事件查看函数的分支预测失败率并进行优化

linux 下/proc/self/statm有当前进程的内存占用情况，
共有七项：vsize虚拟内存页数、resident物理内存、share共享内存、text代码段内存页数， lib引用库内存页数、data_stack 数据/堆栈段内存页数、dt 脏页数，
单位是内存的页数，需要乘以getpagesize()转换为byte。在每个模块结束后统计vsize的增加，可知该模块占用的内存大小

这并不是说“STL必然是坏的”, 编写一个在非优化的build模式下不会变慢10倍的STL实现是有可能的（如EASTL或Libc++）。但是无论出于何种原因，微软的STL速度非常慢。由于它过度依赖于inlining.
作为语言的使用者，我不在乎它是谁的错！我只知道的“STL在调试中太慢”，我要么将它解决掉，要么寻找替代方案（例如不使用STL，重新实现我需要的bits库，或者完全不再使用C++）。

将 cpu 占用率高的线程找出来:
 ps H -eo user,pid,ppid,tid,time,%cpu,cmd --sort=%cpu
这个命令首先指定参数'H'，显示线程相关的信息，格式输出中包含:user,pid,ppid,tid,time,%cpu,cmd，然后再用%cpu字段进行排序。这样就可以找到占用处理器的线程了。
直接 ps Hh -eo pid,tid,pcpu | sort -nk3 |tail 获取对于的进程号和线程号，然后跳转到3.
查看哪个进程线程占用cpu过高； top / ps -aux， 获得进程号
确定哪个线程占用cpu过高，进入进程号的目录：/proc/pid/task， 
执行：grep SleepAVG  **/status  | sort -k2,2 | head，  确定cpu占用较高的线程号。
使用kill -3 pid 会打印线程堆栈的情况
在 Linux 下 top 工具可以显示 cpu 的平均利用率(user,nice,system,idle,iowait,irq,softirq,etc.)，可以显示每个 cpu 的利用率。但是无法显示每个线程的 cpu 利用率情况，这时就可能出现这种情况，总的 cpu 利用率中 user 或 system 很高，但是用进程的 cpu 占用率进行排序时，没有进程的 user 或 system 与之对应。
ps H -eo user,pid,ppid,tid,time,%cpu,cmd --sort=%cpu | grep engine
查看出engine的pid后top -H -p pid直接看线程cpu使用率

proc文件系统
/proc文件系统是一个伪文件系统，它只存在内存当中，而不占用外存空间。
它以文件系统的方式为内核与进程提供通信的接口。用户和应用程序可以通过/proc得到系统的信息，并可以改变内核的某些参数。
由于系统的信息，如进程，是动态改变的，所以用户或应用程序读取/proc目录中的文件时，proc文件系统是动态从系统内核读出所需信息并提交的。
/proc目录中有一些以数字命名的目录，它们是进程目录。
系统中当前运行的每一个进程在/proc下都对应一个以进程号为目录名的目录/proc/pid，它们是读取进程信息的接口。
此外，在Linux2.6.0-test6以上的版本中/proc/pid目录中有一个task目录，/proc/pid/task目录中也有一些以该进程所拥有的线程的线程号命名的目录/proc/pid/task/tid，它们是读取线程信息的接口。

/proc/cpuinfo文件, 该文件中存放了有关 cpu的相关信息(型号，缓存大小等)。
[~]$ cat /proc/cpuinfo
processor       : 0
vendor_id       : GenuineIntel
...
core id         : 0
cpu cores       : 1

/proc/stat文件, 该文件包含了所有CPU活动的信息，该文件中的所有值都是从系统启动开始累计到当前时刻。不同内核版本中该文件的格式可能不大一致，以下通过实例(2.6.24-24版本上)来说明数据该文件中各字段的含义。
:~$cat /proc/stat
cpu  38082 627 27594 89390812256 581 895 0 0
cpu022880 472 16855 430287 10617 576 661 0 0
...

第一行的数值表示的是CPU总的使用情况，所以我们只要用第一行的数字计算就可以了。下表解析第一行各数值的含义：
参数          解析（单位：jiffies）
(jiffies是内核中的一个全局变量，用来记录自系统启动一来产生的节拍数，在linux中，一个节拍大致可理解为操作系统进程调度的最小时间片，不同linux内核可能值有不同，通常在1ms到10ms之间)
user (38082)    从系统启动开始累计到当前时刻，处于用户态的运行时间，不包含 nice值为负进程。
nice (627)      从系统启动开始累计到当前时刻，nice值为负的进程所占用的CPU时间
system (27594)  从系统启动开始累计到当前时刻，处于核心态的运行时间
idle (893908)   从系统启动开始累计到当前时刻，除IO等待时间以外的其它等待时间
iowait (12256) 从系统启动开始累计到当前时刻，IO等待时间(since 2.5.41)
irq (581)           从系统启动开始累计到当前时刻，硬中断时间(since 2.6.0-test4)
softirq (895)      从系统启动开始累计到当前时刻，软中断时间(since2.6.0-test4)
stealstolen(0)    which is the time spent in otheroperating systems when running in a virtualized environment(since 2.6.11)
guest(0)   whichis the time spent running a virtual CPU  for  guest operating systems under the control ofthe Linux kernel(since 2.6.24)
结论2：总的cpu时间totalCpuTime = user + nice+ system + idle + iowait + irq + softirq + stealstolen +  guest

/proc/<pid>/stat文件                                          
该文件包含了某一进程所有的活动的信息，该文件中的所有值都是从系统启动开始累计到当前时刻。以下通过实例数据来说明该文件中各字段的含义。
[ ~]# cat/proc/6873/stat
6873 (a.out) R 6723 6873 6723 34819 6873 8388608 77 0 0 0 41958 31 0 0 25 0 3 05882654 1409024 56 4294967295 134512640 134513720 3215579040 0 2097798 0 0 0 00 0 0 17 0 0 0
说明：以下只解释对我们计算Cpu使用率有用相关参数
参数                              解释
pid=6873                          进程号
utime=1587                       该任务在用户态运行的时间，单位为jiffies
stime=41958                      该任务在核心态运行的时间，单位为jiffies
cutime=0                            所有已死线程在用户态运行的时间，单位为jiffies
cstime=0                            所有已死在核心态运行的时间，单位为jiffies
结论3：进程的总Cpu时间processCpuTime = utime +stime + cutime + cstime，该值包括其所有线程的cpu时间。


--------------------------------------------内存
sbrk不是系统调用，是C库函数。系统调用通常提供一种最小功能，而库函数通常提供比较复杂的功能。
在Linux系统上，程序被载入内存时，内核为用户进程地址空间建立了代码段、数据段和堆栈段，在数据段与堆栈段之间的空闲区域用于动态内存分配。
内核数据结构mm_struct中的成员变量start_code和end_code是进程代码段的起始和终止地址，start_data和 end_data是进程数据段的起始和终止地址，start_stack是进程堆栈段起始地址，start_brk是进程动态内存分配起始地址（堆的起始地址），还有一个 brk（堆的当前最后地址），就是动态内存分配当前的终止地址。
C语言的动态内存分配基本函数是malloc()，在Linux上的基本实现是通过内核的brk系统调用。brk()是一个非常简单的系统调用，只是简单地改变mm_struct结构的成员变量brk的值。
mmap系统调用实现了更有用的动态内存分配功能，可以将一个磁盘文件的全部或部分内容映射到用户空间中，进程读写文件的操作变成了读写内存的操作。在 linux/mm/mmap.c文件的do_mmap_pgoff()函数，是mmap系统调用实现的核心。do_mmap_pgoff()的代码，只是新建了一个vm_area_struct结构，并把file结构的参数赋值给其成员变量m_file，并没有把文件内容实际装入内存。
Linux内存管理的基本思想之一，是只有在真正访问一个地址的时候才建立这个地址的物理映射。
C语言跟内存分配方式
（1） 从静态存储区域分配。内存在程序编译的时候就已经分配好，这块内存在程序的整个运行期间都存在。例如全局变量，static变量。
（2） 在栈上创建。在执行函数时，函数内局部变量的存储单元都可以在栈上创建，函数执行结束时这些存储单元自动被释放。栈内存分配运
算内置于处理器的指令集中，效率很高，但是分配的内存容量有限。
（3）从堆上分配，亦称动态内存分配。程序在运行的时候用malloc或new申请任意多少的内存，程序员自己负责在何时用free或delete释放内存。动态内存的生存期由我们决定，使用非常灵活，但问题也最多
C语言跟内存申请相关的函数主要有 alloc,calloc,malloc,free,realloc,sbrk等.其中alloc是向栈申请内存,因此无需释放. malloc分配的内存是位于堆中的,并且没有初始化内存的内容,因此基本上malloc之后,调用函数memset来初始化这部分的内存空间.calloc则将初始化这部分的内存,设置为0. 而realloc则对malloc申请的内存进行大小的调整.申请的内存最终需要通过函数free来释放. 而sbrk则是增加数据段的大小;
malloc/calloc/free基本上都是C函数库实现的,跟OS无关.C函数库内部通过一定的结构来保存当前有多少可用内存.如果程序 malloc的大小超出了库里所留存的空间,那么将首先调用brk系统调用来增加可用空间,然后再分配空间.free时,释放的内存并不立即返回给os, 而是保留在内部结构中. 可以打个比方: brk类似于批发,一次性的向OS申请大的内存,而malloc等函数则类似于零售,满足程序运行时的要求.这套机制类似于缓冲.
使用这套机制的原因: 系统调用不能支持任意大小的内存分配(有的系统调用只支持固定大小以及其倍数的内存申请,这样的话,对于小内存的分配会造成浪费; 系统调用申请内存代价昂贵,涉及到用户态和核心态的转换.
函数malloc()和calloc()都可以用来分配动态内存空间，但两者稍有区别。   
     malloc()函数有一个参数，即要分配的内存空间的大小：    
     void *malloc(size_t size); 
     calloc()函数有两个参数，分别为元素的数目和每个元素的大小，这两个参数的乘积就是要分配的内存空间的大小：   
     void *calloc(size_t numElements，size_t sizeOfElement)；
     如果调用成功，函数malloc()和calloc()都将返回所分配的内存空间的首地址。
     malloc() 函数和calloc()函数的主要区别是前者不能初始化所分配的内存空间，而后者能。如果由malloc()函数分配的内存空间原来没有被使用过，则其中的每一位可能都是0；反之，如果这部分内存空间曾经被分配、释放和重新分配，则其中可能遗留各种各样的数据。也就是说，使用malloc()函数的程序开始时(内存空间还没有被重新分配)能正常运行，但经过一段时间后(内存空间已被重新分配)可能会出现问题。
     calloc() 函数会将所分配的内存空间中的每一位都初始化为零，也就是说，如果你是为字符类型或整数类型的元素分配内存，那么这些元素将保证会被初始化为零；如果你是为指针类型的元素分配内存，那么这些元素通常(但无法保证)会被初始化为空指针；如果你是为实数类型的元素分配内存，那么这些元素可能(只在某些计算机中)会被初始化为浮点型的零。

内存进程使用分类查看USS/PSS
smem

mallopt(M_MMAP_THRESHOLD, 0);
mallopt(M_MMAP_MAX, 1e9);//超大
malloc_stats();
malloc_trim(0);

bin链中的chunk均为free chunk
Fast Bin
概念：chunk的大小在32字节~128字节（0x20~0x80）的chunk称为“fast chunk”（大小不是malloc时的大小，而是在内存中struct malloc_chunk的大小，包含前2个成员）
fast bin链表的个数为10个
不会对free chunk进行合并：鉴于设计fast bin的初衷就是进行快速的小内存分配和释放，因此系统将属于fast bin的chunk的PREV_INUSE位总是设置为1，这样即使当fast bin中有某个chunk同一个free chunk相邻的时候，系统也不会进行自动合并操作，而是保留两者。这样做可能会造成额外的碎片化问题
fast bin是单链表，Unsorted/Small/Large 都是双向链表

cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Debuginfo.repo
enable=0改为enable=1，还不行
yum --nogpgcheck --enablerepo=debug install glibc-debuginfo
yum install yum-utils
debuginfo-install glibc
git clone https://github.com/cloudburst/libheap
pip3 install --user ./libheap/
wget https://github.com/cloudburst/libheap/archive/master.zip
cd libheap
python setup.py install
vi /usr/lib/python2.7/site-packages/libheap/libheap.cfg
[Glibc]
version = 2.17
glibc版本
ldd --version
查得 gitinit 文件的路径：
/etc/gdb/gitinit
在该文件中添加如下指令：
# System-wide GDB initialization file.
python import sys
python sys.path.append("/usr/lib/python2.7/site-packages/")
python from libheap import *

heap -l
Arena(s) found:
  arena @ 0x7fc61a9cf760
  arena @ 0x7fc600000020
  arena @ 0x7fc608000020

p *(mchunkptr) 0x7fc61a9cf760
$1 = struct malloc_chunk {      //存储堆数据信息的结构体, 主Arena没有多个堆，因此没有heap_info结构
prev_size   = 0x0
size        = 0x49124790
fd          = 0x0
bk          = 0x0
fd_nextsize = 0x0
bk_nextsize = 0x54c039e0

p *(mchunkptr) 0x7fc600000020
$2 = struct malloc_chunk {
prev_size   = 0x200000000
size        = 0x7fc600003350
fd          = 0x7fc600002a70
bk          = 0x0
fd_nextsize = 0x7fc600002a20
bk_nextsize = 0x7fc600002de0

p main_arena
$3 = struct malloc_state {
mutex            = 0x0
flags            = 0x0
fastbinsY        = {...}
top              = 0x68db6130   //指向top chunk
last_remainder   = 0x35b0b110   //指向切割后剩下的last reminder
bins             = {...}        //smallbins，unsortedbin，largebins
binmap           = {...}        //系统查看有哪些箱链中有块时，不去fastbinsY和箱数组一个一个的遍历, 通过binmap变量，采用二进制存储，将二进制位与数组的索引相对
next             = 0x7fc600000020
next_free        = 0x0
attached_threads = 0x1          //引用当前arena的线程数量
system_mem       = 0x668ae000   //记录当前获取的系统内存
max_system_mem   = 0x668ae000
(gdb) p mp_
$4 = struct malloc_par {
trim_threshold   = 0x602000 //设置topchunk的最大值，超过这个值就要把top chunk返还给操作系统
top_pad          = 0x20000
mmap_threshold   = 0x301000 //申请的内存超过这个值，就要使用mmap分配内存
arena_test       = 0x8
arena_max        = 0x0
n_mmaps          = 0x1c     //当前mmap的数量
n_mmaps_max      = 0x10000  //mmap数量的最大值
max_n_mmaps      = 0x1e
no_dyn_threshold = 0x0
mmapped_mem      = 0x6259000 //当前mmap的总大小
max_mmapped_mem  = 0x685b000
max_total_mem    = 0x0
sbrk_base        = 0x254b000




------------------------------------------------网络
Linux Kernel 的 TCP/IP 协议栈提供了一组控制参数用于配置 TCP 端口 的快速回收重用，当把它们的值设置为1时表示启用该选项： 
1) net.ipv4.tcp_tw_reuse = 1 
2) net.ipv4.tcp_tw_recycle = 1 
3) net.ipv4.tcp_timestamps = 1（tcp_tw_recycle启用时必须同时启用本项， 反之则不然，timestamps 用于 RTT 计算，在 TCP 报文头部的可选项中传输，两个参数分别为发送方发送TCP报文时的时间戳和接收方收到TCP报文 响应时的时间戳。Linux系统和移动设备上的Android、iOS都缺省开启了此选 项，建议不要随意关闭） 
 
IOS七层协议：
物理层：传输的是比特流，网卡位于这层。
数据链路层：本层传输的是帧；本层主要定义了如何格式化数据，错误检测。交换机位于本层
网络层：本层传输的是数据包，路由器位于本层。本层协议是IP协议（Internet Protocol Address），主要功能是路由选择最短路径，将数据包从发送端路由到接收端
传输层：协议有TCP（传输控制协议）/UDP（用户数据报协议）；主要是控制重传、数据分割之类的，主要是解决数据之间的传输，和传输质量。是IOS最核心的一层，其中TCP协议是最重要的协议
会话层：不同机器之间建立会话
表示层：解决不同系统之间的语法问题
应用层：应用网络中发送数据：需要注意Http协议（超文本传输协议），Https（超文本传输安全协议）
TCP/IP四层模型
与IOS七层模型相同，从下往上依次为：链路层（物理层+数据链路层），网络层（IP），传输层（TCP），应用层（应用层+表示层+会话层）（HTTP）。

recv函数
int recv( SOCKET s, char FAR *buf, int len, int flags);
不论是客户还是服务器应用程序都用recv函数从TCP连接的另一端接收数据。
该函数的第一个参数指定接收端套接字描述符； 
第二个参数指明一个缓冲区，该缓冲区用来存放recv函数接收到的数据； 
第三个参数指明buf的长度； 第四个参数一般置0。
这里只描述同步Socket的recv函数的执行流程。当应用程序调用recv函数时，
（1）recv先等待s的发送缓冲中的数据被协议传送完毕，如果协议在传送s的发送缓冲中的数据时出现网络错误，那么recv函数返回SOCKET_ERROR，
（2）如果s的发送缓冲中没有数据或者数据被协议成功发送完毕后，recv先检查套接字s的接收缓冲区，
如果s接收缓冲区中没有数据或者协议正在接收数 据，那么recv就一直等待，直到协议把数据接收完毕。
当协议把数据接收完毕，recv函数就把s的接收缓冲中的数据copy到buf中
（注意协议接收到的数据可能大于buf的长度，所以 在这种情况下要调用几次recv函数才能把s的接收缓冲中的数据copy完。
recv函数仅仅是copy数据，真正的接收数据是协议来完成的）， recv函数返回其实际copy的字节数。
如果recv在copy时出错，那么它返回SOCKET_ERROR；
如果recv函数在等待协议接收数据时网络中断了，那么它返回0。
默认 socket 是阻塞的 解阻塞与非阻塞recv返回值没有区分，都是 <0 出错 ；=0 连接关闭 ；>0 接收到数据大小，
特别：
返回值<0时并且(errno == EINTR || errno == EWOULDBLOCK || errno == EAGAIN)的情况下认为连接是正常的，继续接收。
只是阻塞模式下recv会阻塞着接收数据，非阻塞模式下如果没有数据会返回，不会阻塞着读，因此需要循环读取）。
返回说明： 
成功执行时，返回接收到的字节数。
另一端已关闭则返回0。
失败返回-1，
errno被设为以下的某个值 
EAGAIN：套接字已标记为非阻塞，而接收操作被阻塞或者接收超时 
EBADF：sock不是有效的描述词 
ECONNREFUSE：远程主机阻绝网络连接 
EFAULT：内存空间访问出错 
EINTR：操作被信号中断 
EINVAL：参数无效 
ENOMEM：内存不足 
ENOTCONN：与面向连接关联的套接字尚未被连接上 
ENOTSOCK：sock索引的不是套接字 当返回值是0时，为正常关闭连接；
当对侧没有send，即本侧的套接字s的接收缓冲区无数据，返回值是什么（EAGAIN，原因为超时，待测）


------------------------------------------------工具软件
gcc升级到9.2.0
gcc -v    # 查看当前gcc版本信息
yum -y install wget bzip2 gcc gcc-c++ glibc-headers
wget -c -P /opt/tmp/ https://mirrors.tuna.tsinghua.edu.cn/gnu/gcc/gcc-9.2.0/gcc-9.2.0.tar.gz
cd /opt/tmp/
tar -zxvf gcc-9.2.0.tar.gz
cd gcc-9.2.0
./contrib/download_prerequisites    #下载gmp mpfr mpc等供编译需求的依赖项
mkdir build    #不能在source目录下configure必须在上一层的目录下
cd build
../configure --prefix=/usr/local/gcc-9.2.0 --enable-bootstrap --enable-checking=release --enable-languages=c,c++ --disable-multilib
make -j 16 && make install
gcc -v
echo -e '\nexport PATH=/usr/local/gcc-9.2.0/bin:$PATH\n' >> /etc/profile.d/gcc.sh && source /etc/profile.d/gcc.sh
#导出头文件
ln -sv /usr/local/gcc-9.2.0/include/ /usr/include/gcc
#配置生效
ldconfig -v
#导出验证
ldconfig -p |grep gcc
gcc -v
link到旧的gcc
/usr/bin/gcc

/usr/bin/gcc -v

备份系统默认的gcc版本
mv /usr/bin/gcc /usr/bin/gcc-bak
mv /usr/bin/g++ /usr/bin/g++-bak
mv /usr/bin/c++ /usr/bin/c++-bak
mv /usr/lib64/libstdc++.so.6 /usr/lib64/libstdc++.so.6-bak
创建新的gcc软连接
ln -fs /usr/local/gcc-9.2.0/bin/gcc /usr/bin/gcc
ln -fs /usr/local/gcc-9.2.0/bin/c++ /usr/bin/c++
ln -fs /usr/local/gcc-9.2.0/bin/g++ /usr/bin/g++
ln -fs /usr/local/gcc-9.2.0/lib64/libstdc++.so.6.0.27 /usr/lib64/libstdc++.so.6

cmake手动升级
yum remove cmake
cd /opt
wget https://cmake.org/files/v3.10/cmake-3.10.2-Linux-x86_64.tar.gz
tar zxvf cmake-3.10.2-Linux-x86_64.tar.gz
./bootstrap
报错ssl的话安装一下
yum install openssl-devel
make
make install
错误2：make install时出现报错：
[100%] Built target foo
Install the project...
-- Install configuration: ""
CMake Error at Source/kwsys/cmake_install.cmake:41 (file):
  file cannot create directory: /usr/local/doc/cmake-3.16/cmsys.  Maybe need
  administrative privileges.
Call Stack (most recent call first):
  cmake_install.cmake:42 (include)
Makefile:76: recipe for target 'install' failed
make: *** [install] Error 1
解决方案：
直接在make 和 make install 前面加 sudo
vi /etc/profile
export CMAKE_HOME=/data/download/CMake-3.18.2
export PATH=$PATH:$CMAKE_HOME/bin
source /etc/profile

svn
yum install apr
yum install apr-util-devel
wget www.sqlite.org/sqlite-amalgamation-3071501.zip
unzip sqlite-amalgamation-3071501.zip
mv  sqlite-amalgamation-3071501  sqlite-amalgamation
yum install cyrus-sasl-md5
yum install cyrus-sasl cyrus-sasl-plain cyrus-sasl-ldap
svn checkout svn://ccc.rdev.ccc.net/ccc/trunk/Server/work ./
svn update


jq
centos
yum install jq -y

wget https://github.com/stedolan/jq/releases/download/jq-1.5/jq-1.5.tar.gz
tar -zvxf jq-1.5.tar.gz
jq-1.5
./configure --prefix=/usr/local --disable-maintainer-mode
make && make install


centos开机自启动
# chkconfig:   2345 90 10
# description:  server run env
1、将脚本移动到/etc/rc.d/init.d目录下
mv /opt/script/AutoServices.sh /etc/rc.d/init.d
2、增加脚本的可执行权限
chmod +x  /etc/rc.d/init.d/AutoServices.sh
3、添加脚本到开机自动启动项目中
cd /etc/rc.d/init.d
chkconfig --add AutoServices.sh
chkconfig AutoServices.sh on
方式二，
chmod +x /etc/rc.d/rc.local
./data/AutoServices.sh start


--------------------------------------------------注意事项
### libc不建议静态链接
The most important reason why glibc should not be statically linked, is that it makes extensive internal use of dlopen, 
to load NSS (Name Service Switch) modules and iconv conversions. The modules themselves refer to C library functions. 
If the main program is dynamically linked with the C library, that's no problem. But if the main program is statically linked with the C library, 
dlopen has to go load a second copy of the C library to satisfy the modules' load requirements.

This means your "statically linked" program still needs a copy of libc.so.6 to be present on the file system, 
plus the NSS or iconv or whatever modules themselves, plus other dynamic libraries that the modules might need, like ld-linux.so.2, libresolv.so.2, etc. 
This is not what people usually want when they statically link programs.

It also means the statically linked program has two copies of the C library in its address space, and they might fight over whose stdout buffer is to be used, 
who gets to call sbrk with a nonzero argument, that sort of thing. There is a bunch of defensive logic inside glibc to try to make this work, 
but it's never been guaranteed to work.

You might think your program doesn't need to worry about this because it doesn't ever call getaddrinfo or iconv, 
but locale support uses iconv internally, which means any stdio.h function might trigger a call to dlopen, and you don't control this, 
the user's environment variable settings do.

如果有多份libc的拷贝，程序将不使用brk，直接用mmap分配main arena

### fork要特别小心，特别是多线程环境，有互斥量的情况

